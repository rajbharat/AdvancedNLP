{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Andy Lau (RNN scratch).ipynb","provenance":[{"file_id":"1gmFQhlL2phfWmhmZ6EJbFUmpxx3zQSI8","timestamp":1582101041580}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"IlP_vOD7Sleq","colab_type":"code","colab":{}},"source":["!pip install mxnet==1.6.0b20200101\n","!pip install d2lzh==0.8.10\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgvJqwjTSftI","colab_type":"code","outputId":"a0ef63f7-8d4e-4aab-99be-acccc60c3752","executionInfo":{"status":"ok","timestamp":1585212640920,"user_tz":-480,"elapsed":2739,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from mxnet import nd\n","import random\n","import zipfile\n","\n","with open('/content/drive/My Drive/Data/Andy_Lau.txt') as f:\n","  for line in f:\n","    corpus_chars = f.read()\n","\n","corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n","\n","# We map each character to a continuous integer starting from 0, also known as the index, to facilitate subsequent data processing. \n","# In order to get the index, we take all the different characters in the data set, and then map them one by one to the index to construct the dictionary. \n","# Next, print vocab_size, which is the number of different characters in the dictionary, also known as the vocabulary size.\n","idx_to_char = list(set(corpus_chars))\n","char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n","vocab_size = len(char_to_idx)\n","vocab_size\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2783"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"gIZU5vbNfV_G","colab_type":"code","outputId":"f6da1b4d-9e8b-4757-f761-461e63d725ed","executionInfo":{"status":"ok","timestamp":1585212648031,"user_tz":-480,"elapsed":588,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Each character in the training data set is converted into an index, and the first 20 characters and their corresponding indexes are printed.\n","corpus_indices = [char_to_idx[char] for char in corpus_chars]\n","sample = corpus_indices[:20]\n","print('chars:', ''.join([idx_to_char[idx] for idx in sample]))\n","print('indices:', sample)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["chars: 但是不敌愁眉难服气为你不肯心死 如何玩味\n","indices: [1665, 455, 2030, 224, 1276, 735, 53, 1103, 2241, 277, 819, 2030, 1035, 1852, 1940, 584, 1178, 524, 1509, 258]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VWfmFHtygnC8","colab_type":"code","outputId":"69f0bcf0-9541-4559-a60a-97548cb5be81","executionInfo":{"status":"ok","timestamp":1585212657674,"user_tz":-480,"elapsed":615,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import math\n","from mxnet import autograd, nd\n","from mxnet.gluon import loss as gloss\n","import time\n","\n","# One-hot encoding\n","# To represent words as vectors into a neural network, a simple way is to use one-hot vectors. Suppose the number of different characters in the dictionary is N (i.e. vocab_size), \n","# each character has a one-to-one correspondence with a continuous integer value index from 0 to N−1.\n","# If the index of a character is an integer i, then we create a vector of length N with all 0s, and set the element whose position is i to 1.\n","# This vector is a one-hot vector to the original character. The following shows one-hot vectors with indices 0 and 2, respectively. The vector length is equal to the dictionary size.\n","\n","nd.one_hot(nd.array([0, 2]), vocab_size)\n","\n","# The shape of our mini-batch for each sample is (batch size, number of time steps). \n","# The following function transforms such small batch into several matrices that can be input into the network. Their shape is (batch size, dictionary size). The number of matrices is equal to the number of time steps.\n","# That is, the input of time step t is Xt ∈ Rn × d, where n is the batch size and d is the number of inputs, i.e. the length of the one-hot vector (dictionary size).\n","\n","def to_onehot(X, size):\n","    return [nd.one_hot(x, size) for x in X.T]\n","\n","X = nd.arange(10).reshape((2, 5))\n","inputs = to_onehot(X, vocab_size)\n","len(inputs), inputs[0].shape\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5, (2, 2783))"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"btMOaqILinV2","colab_type":"code","outputId":"f82b17ea-ea54-4142-d977-d6620ea102f4","executionInfo":{"status":"ok","timestamp":1585212663133,"user_tz":-480,"elapsed":1410,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Next, we initialize the model parameters. The number of hidden units num_hiddens is a hyperparameter.\n","\n","import d2lzh as d2l\n","\n","num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n","ctx = d2l.try_gpu()\n","print('will use', ctx)\n","\n","def get_params():\n","    def _one(shape):\n","        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n","\n","    # Hidden layer parameters\n","    W_xh = _one((num_inputs, num_hiddens))\n","    W_hh = _one((num_hiddens, num_hiddens))\n","    b_h = nd.zeros(num_hiddens, ctx=ctx)\n","    # Output layer parameters\n","    W_hq = _one((num_hiddens, num_outputs))\n","    b_q = nd.zeros(num_outputs, ctx=ctx)\n","    # Attach gradient\n","    params = [W_xh, W_hh, b_h, W_hq, b_q]\n","    for param in params:\n","        param.attach_grad()\n","    return params"],"execution_count":0,"outputs":[{"output_type":"stream","text":["will use cpu(0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LrrSYbc8i7pf","colab_type":"code","colab":{}},"source":["# We implement this model based on the computational expression of a recurrent neural network. First define the init_rnn_state function to return the initialized hidden state. \n","# It returns a tuple consisting of an NDArray whose shape is (batch size, number of hidden units). Tuples are used to make it easier to handle cases where the hidden state contains multiple NDArrays.\n","\n","def init_rnn_state(batch_size, num_hiddens, ctx):\n","    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )\n","\n","# The following rnn function defines how to calculate the hidden state and output in a time step. The activation function here uses the tanh function. \n","# When the elements are evenly distributed in the number domain, the mean value of the tanh function is 0.\n","\n","def rnn(inputs, state, params):\n","    # inputs and outputs are both matrices of shape (batch_size, vocab_size). There are num_steps matrices.\n","    W_xh, W_hh, b_h, W_hq, b_q = params\n","    H, = state\n","    outputs = []\n","    for X in inputs:\n","        H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h)\n","        Y = nd.dot(H, W_hq) + b_q\n","        outputs.append(Y)\n","    return outputs, (H,)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rv3TR6VdjHka","colab_type":"code","outputId":"57911399-95ee-4c2e-c0a4-a6c7be6e1857","executionInfo":{"status":"ok","timestamp":1585212675020,"user_tz":-480,"elapsed":591,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# A simple test to observe the number of output results (the number of time steps), the shape of the output layer output at the first time step, and the shape of the hidden state.\n","\n","state = init_rnn_state(X.shape[0], num_hiddens, ctx)\n","inputs = to_onehot(X.as_in_context(ctx), vocab_size)\n","params = get_params()\n","outputs, state_new = rnn(inputs, state, params)\n","len(outputs), outputs[0].shape, state_new[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5, (2, 2783), (2, 256))"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"1uvZn2ffjOps","colab_type":"code","colab":{}},"source":["# Defining prediction functions\n","# The following function predicts the next num_chars characters based on the prefix (a string containing several characters). \n","# It sets the recurrent neural unit rnn as a function parameter.\n","\n","def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n","                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n","    state = init_rnn_state(1, num_hiddens, ctx)\n","    output = [char_to_idx[prefix[0]]]\n","    for t in range(num_chars + len(prefix) - 1):\n","        # Use the output of the previous step as the input of the current step\n","        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n","        # Calculate output and update hidden status\n","        (Y, state) = rnn(X, state, params)\n","        # Input for the next time step is the character in prefix or the current best predicted character\n","        if t < len(prefix) - 1:\n","            output.append(char_to_idx[prefix[t + 1]])\n","        else:\n","            output.append(int(Y[0].argmax(axis=1).asscalar()))\n","    return ''.join([idx_to_char[i] for i in output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzOOtStUjTrF","colab_type":"code","outputId":"0fb64d93-4b78-47d8-da7e-e86d2e803d17","executionInfo":{"status":"ok","timestamp":1585212683615,"user_tz":-480,"elapsed":1395,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# To test the predict_rnn function. We will create a 10-character lyrics (regardless of the prefix length) based on the prefix \"男人\". Because the model parameters are random values, the prediction results are also random.\n","\n","predict_rnn('男人', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n","            ctx, idx_to_char, char_to_idx)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'男人剧性漓然畅巅捕祇牺延'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"mvwytOSgjdhs","colab_type":"code","colab":{}},"source":["# Clipping Gradients\n","# Gradient decay or gradient explosion is more likely to occur in recurrent neural networks. To deal with gradient explosions, we can clip gradients. Suppose we stitch all the elements of the model parameter gradients into a vector g and set the threshold of the cropping as θ.\n","\n","def grad_clipping(params, theta, ctx):\n","    norm = nd.array([0], ctx)\n","    for param in params:\n","        norm += (param.grad ** 2).sum()\n","    norm = norm.sqrt().asscalar()\n","    if norm > theta:\n","        for param in params:\n","            param.grad[:] *= theta / norm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_SwFl7ajmVH","colab_type":"code","colab":{}},"source":["# We use perplexity to evaluate the quality of a language model. Perplexity is the value obtained from exponential operation from the cross-entropy loss function.\n","\n","# In the best case, the model always predicts the probability of the label category as 1, and the perplexity degree is 1;\n","# In the worst case, the model always predicts the probability of the label category as 0, and the perplexity is positive infinity;\n","# In the baseline case, the model always predicts that the probability is the same for all categories. The degree of perplexity is the number of categories.\n","# Obviously, the perplexity of any valid model must be less than the number of categories. In this example, the perplexity must be less than vocab_size."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"86GOsWw0j6dT","colab_type":"code","colab":{}},"source":["# Defining model training functions\n","\n","# Evaluate the model using perplexity.\n","# Clip gradient before iterating model parameters.\n","# Different sampling methods for time series data will lead to different initialization of hidden state.\n","\n","\n","def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n","                          vocab_size, ctx, corpus_indices, idx_to_char,\n","                          char_to_idx, is_random_iter, num_epochs, num_steps,\n","                          lr, clipping_theta, batch_size, pred_period,\n","                          pred_len, prefixes):\n","    if is_random_iter:\n","        data_iter_fn = d2l.data_iter_random\n","    else:\n","        data_iter_fn = d2l.data_iter_consecutive\n","    params = get_params()\n","    loss = gloss.SoftmaxCrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        if not is_random_iter:\n","            # If using adjacent sampling, initialize the hidden state at the beginning of the epoch\n","            state = init_rnn_state(batch_size, num_hiddens, ctx)\n","        l_sum, n, start = 0.0, 0, time.time()\n","        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n","        for X, Y in data_iter:\n","            if is_random_iter:\n","            # If using random sampling, initialize the hidden state before each mini-batch update\n","                state = init_rnn_state(batch_size, num_hiddens, ctx)\n","            else:\n","            # Otherwise need to use the detach function to detach the hidden state from the calculation graph \n","                for s in state:\n","                    s.detach()\n","            with autograd.record():\n","                inputs = to_onehot(X, vocab_size)\n","                # outputs num_steps matrices which shape of (batch_size, vocab_size)\n","                (outputs, state) = rnn(inputs, state, params)\n","                # The shape after stitching is (num_steps * batch_size, vocab_size)\n","                outputs = nd.concat(*outputs, dim=0)\n","                # The shape of Y is (batch_size, num_steps). After transposing, it becomes a length of \n","                # batch * num_steps vector, which corresponds to the output rows one by one\n","                y = Y.T.reshape((-1,))\n","                # Calculate average classification error using cross entropy loss\n","                l = loss(outputs, y).mean()\n","            l.backward()\n","            grad_clipping(params, clipping_theta, ctx)\n","            # Clip gradient\n","            d2l.sgd(params, lr, 1)\n","            # Because the error has been averaged, the gradient need not be averaged\n","\n","            l_sum += l.asscalar() * y.size\n","            n += y.size\n","\n","        if (epoch + 1) % pred_period == 0:\n","            print('epoch %d, perplexity %f, time %.2f sec' % (\n","                epoch + 1, math.exp(l_sum / n), time.time() - start))\n","            for prefix in prefixes:\n","                print(' -', predict_rnn(\n","                    prefix, pred_len, rnn, params, init_rnn_state,\n","                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4ll0RPZkF2f","colab_type":"code","colab":{}},"source":["# Train models and write lyrics\n","# First, set the model hyperparameters. We will create a piece of lyrics with a length of 50 characters (regardless of the prefix length) based on the prefixes \"男人\" and \"女人“\". \n","## Every 50 iterations we write a lyrics based on the currently trained model.\n","\n","num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n","pred_period, pred_len, prefixes = 50, 50, ['男人', '女人']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qREEoeIDkObz","colab_type":"code","outputId":"d203d784-32db-445b-8a8c-97ea7b4c986b","executionInfo":{"status":"ok","timestamp":1583744965184,"user_tz":-480,"elapsed":23488940,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":237}},"source":["train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n","                      vocab_size, ctx, corpus_indices, idx_to_char,\n","                      char_to_idx, True, num_epochs, num_steps, lr,\n","                      clipping_theta, batch_size, pred_period, pred_len,\n","                      prefixes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 50, perplexity 53.750615, time 101.17 sec\n"," - 男人的心里 我的爱你我的地 我的天不开你的柔情 我的心情感动 我的泪却是我的家 你的眼神我的心 我的心不\n"," - 女人 我不是你的心 我的心不是你的眼光 我的心不可以 只管你一天 我的心忘了我的心 我的爱你我的一切 我\n","epoch 100, perplexity 25.935840, time 97.77 sec\n"," - 男人究竟不是我的心 你的爱该那么的不会 我踏上了爱你的心中 曾经错失意中 你曾经以不对你 我是我的心也有\n"," - 女人 从未最相信相见 但愿现在未来 什么时候才明白生命里面 我为何你的身影 你去爱我的心曾有爱你 我是真\n","epoch 150, perplexity 16.976173, time 99.71 sec\n"," - 男人究竟为了什么 因为我有感到也不要多么好 天生命中不能不停 我的心情已不够 我这样对我的爱你 我已经不\n"," - 女人 要不敢爱惜你是我痴心的爱情 我已经不可不可以 只想你的心 无法的风我们都是你 我不能忘记你我的眼中\n","epoch 200, perplexity 12.711507, time 98.90 sec\n"," - 男人究竟犯了心 你在我的爱你最后一生不讲千百句 言说谎言却不能隐藏 我的胸前安歇的一切最美一个男孩手走在\n"," - 女人 既然大家没有错 只有你何必说话 就说一生都可以 平静的发现我的一切变得一个梦 经过多少的心中有你的\n"],"name":"stdout"}]}]}