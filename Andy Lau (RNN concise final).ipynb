{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Andy Lau (RNN concise final).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPne8rDxDQUZfuxuZR+5nfB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zGawyYjjxOwg","colab_type":"code","colab":{}},"source":["!pip install mxnet==1.6.0b20200101\n","!pip install d2lzh==0.8.10\n","\n","import d2lzh as d2l\n","import math\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, rnn\n","import time\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbdHSXmlxvlS","colab_type":"code","outputId":"e6ea7987-48fa-4096-e69a-8db4e505521e","executionInfo":{"status":"ok","timestamp":1585270140650,"user_tz":-480,"elapsed":63554,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from mxnet import nd\n","import random\n","\n","with open('/content/drive/My Drive/Data/Andy_Lau.txt') as f:\n","  for line in f:\n","    corpus_chars = f.read()#.decode('utf-8')\n","\n","# This data set has many characters. For printing convenience, we replace line breaks with spaces\n","corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n","# corpus_chars = corpus_chars[0:10000]\n","\n","# We map each character to a continuous integer starting from 0, also known as the index, to facilitate subsequent data processing. \n","# In order to get the index, we take all the different characters in the data set, and then map them one by one to the index to construct the dictionary. \n","# Next, print vocab_size, which is the number of different characters in the dictionary, also known as the vocabulary size.\n","idx_to_char = list(set(corpus_chars))\n","char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n","vocab_size = len(char_to_idx)\n","corpus_indices = [char_to_idx[char] for char in corpus_chars]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZnYddRnhyH0S","colab_type":"code","outputId":"6221420a-64b0-42cd-8a70-ca4c911a45c0","executionInfo":{"status":"ok","timestamp":1585270146012,"user_tz":-480,"elapsed":1776,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Defining the model\n","# Gluon's rnn module provides an implementation of a recurrent neural network. Below, construct a recurrent neural network layer rnn_layer with a single hidden layer and 256 hidden units, and initialize the weights.\n","\n","num_hiddens = 256\n","rnn_layer = rnn.RNN(num_hiddens)\n","rnn_layer.initialize()\n","\n","# Next call the member function begin_state of rnn_layer to return the list of initialized hidden states. It has an element with the shape (number of hidden layers, batch size, number of hidden units).\n","\n","batch_size = 2\n","state = rnn_layer.begin_state(batch_size=batch_size)\n","state[0].shape\n","\n","# The input shape of rnn_layer is (number of time steps, batch size, number of inputs). The number of inputs is the one-hot vector length (vocabulary size). In addition, rnn_layer, as the rnn.RNN instance of Gluon, will return the output and hidden state respectively after the forward calculation. \n","# The output refers to the hidden state calculated and output by the hidden layer at each step. They are usually used as the input to the following output layer. \n","\n","num_steps = 35\n","X = nd.random.uniform(shape=(num_steps, batch_size, vocab_size))\n","Y, state_new = rnn_layer(X, state)\n","Y.shape, len(state_new), state_new[0].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((35, 2, 256), 1, (1, 2, 256))"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"ZBHfI-CqyXAB","colab_type":"code","colab":{}},"source":["# Next it inherits the Block class to define a complete recurrent neural network. It first takes the input data, uses a one-hot vector representation, then inputs it into rnn_layer, and then uses the fully connected output layer to get the output. \n","# The number of outputs is equal to the vocab_size.\n","\n","class RNNModel(nn.Block):\n","    def __init__(self, rnn_layer, vocab_size, **kwargs):\n","        super(RNNModel, self).__init__(**kwargs)\n","        self.rnn = rnn_layer\n","        self.vocab_size = vocab_size\n","        self.dense = nn.Dense(vocab_size)\n","\n","    def forward(self, inputs, state):\n","        # Get the one-hot vector representation after transposing the input to (num_steps, batch_size)\n","        X = nd.one_hot(inputs.T, self.vocab_size)\n","        Y, state = self.rnn(X, state)\n","\n","        # The fully connected layer will first change the shape of Y to (num_steps * batch_size, num_hiddens), and its output shape will be (num_steps * batch_size, vocab_size)\n","        output = self.dense(Y.reshape((-1, Y.shape[-1])))\n","        return output, state\n","\n","    def begin_state(self, *args, **kwargs):\n","        return self.rnn.begin_state(*args, **kwargs)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JAzYl6gTycEJ","colab_type":"code","outputId":"eb4b4840-cdef-41aa-d374-74eb2c606a6c","executionInfo":{"status":"ok","timestamp":1585270163450,"user_tz":-480,"elapsed":1733,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Training the Model\n","# A prediction function is defined below. The difference here is the function interface for forward computation and initialization of hidden states.\n","\n","def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char,\n","                      char_to_idx):\n","    # Use model member functions to initialize the hidden state\n","    state = model.begin_state(batch_size=1, ctx=ctx)\n","    output = [char_to_idx[prefix[0]]]\n","    for t in range(num_chars + len(prefix) - 1):\n","        X = nd.array([output[-1]], ctx=ctx).reshape((1, 1))\n","        (Y, state) = model(X, state)\n","        # Forward calculation does not need to pass in model parameters\n","        if t < len(prefix) - 1:\n","            output.append(char_to_idx[prefix[t + 1]])\n","        else:\n","            output.append(int(Y.argmax(axis=1).asscalar()))\n","    return ''.join([idx_to_char[i] for i in output])\n","\n","# Predict using a model with random weights.\n","\n","ctx = d2l.try_gpu()\n","model = RNNModel(rnn_layer, vocab_size)\n","model.initialize(force_reinit=True, ctx=ctx)\n","predict_rnn_gluon('男人', 10, model, vocab_size, ctx, idx_to_char, char_to_idx)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'男人褓劳墙净颇液液液液液'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"2g25YDYnyncL","colab_type":"code","colab":{}},"source":["# Implement the training function. Here only adjacent samples are used to read the data.\n","\n","def train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n","                                corpus_indices, idx_to_char, char_to_idx,\n","                                num_epochs, num_steps, lr, clipping_theta,\n","                                batch_size, pred_period, pred_len, prefixes):\n","    loss = gloss.SoftmaxCrossEntropyLoss()\n","    model.initialize(ctx=ctx, force_reinit=True, init=init.Normal(0.01))\n","    trainer = gluon.Trainer(model.collect_params(), 'sgd',\n","                            {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n","\n","    for epoch in range(num_epochs):\n","        l_sum, n, start = 0.0, 0, time.time()\n","        data_iter = d2l.data_iter_consecutive(\n","            corpus_indices, batch_size, num_steps, ctx)\n","        state = model.begin_state(batch_size=batch_size, ctx=ctx)\n","        for X, Y in data_iter:\n","            for s in state:\n","                s.detach()\n","            with autograd.record():\n","                (output, state) = model(X, state)\n","                y = Y.T.reshape((-1,))\n","                l = loss(output, y).mean()\n","            l.backward()\n","            # Clip gradient\n","            params = [p.data() for p in model.collect_params().values()]\n","            d2l.grad_clipping(params, clipping_theta, ctx)\n","            trainer.step(1)\n","            # Because the error has been averaged, the gradient need not be averaged\n","            l_sum += l.asscalar() * y.size\n","            n += y.size\n","\n","        if (epoch + 1) % pred_period == 0:\n","            print('epoch %d, perplexity %f, time %.2f sec' % (\n","                epoch + 1, math.exp(l_sum / n), time.time() - start))\n","            for prefix in prefixes:\n","                print(' -', predict_rnn_gluon(\n","                    prefix, pred_len, model, vocab_size, ctx, idx_to_char,\n","                    char_to_idx))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqoVqg3iytRx","colab_type":"code","outputId":"f6d31bc2-9905-4004-c569-e9f569fe3e5a","executionInfo":{"status":"ok","timestamp":1585282572871,"user_tz":-480,"elapsed":1111354,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":293}},"source":["# Train the model\n","\n","num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e2, 1e-2\n","pred_period, pred_len, prefixes = 50, 50, ['男人', '女人']\n","train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx, \n","                            corpus_indices, idx_to_char, char_to_idx,\n","                            num_epochs, num_steps, lr, clipping_theta,\n","                            batch_size, pred_period, pred_len, prefixes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 50, perplexity 65.720982, time 48.47 sec\n"," - 男人 我是你的梦里 我不要再不想要再一次 我是我的温柔 你是我的温柔 你的笑你 我的心不是你的口我是我的\n"," - 女人 我是我最好的朋友 一天真的爱走的理 我的心不是你的口 我的家不是你的口 我的眼不是你的泪 我的人不\n","epoch 100, perplexity 36.926451, time 49.37 sec\n"," - 男人 我要你的心里 爱你的爱意 我的眼神 你的眼光在我的温柔 我的眼光是你的路 我的眼睛是你的泪 我知道\n"," - 女人 就像是我们也是不是有太多歉 我要求你愿我一切 一对如一天走一次 只有爱的苦口 我们都在这方情义无可\n","epoch 150, perplexity 26.287890, time 48.85 sec\n"," - 男人 谁会懂得我太多我的心已经在人 你是我的女人 我说的心都跟你 不知道为了错的爱情 情深深刻 这一生都\n"," - 女人 谁又怕的心 没有再会再独留过 我们都在感慨中等候 我们都在感慨之中 我是我的心中 我的心也有苦苦嘘\n","epoch 200, perplexity 20.522814, time 49.34 sec\n"," - 男人 谁会懂得不相爱 我会愿意一样 我要是我的心情 我的眼神么的眼风 我有天空开不可惜 我会想番起我由以\n"," - 女人 如何能相逢 你是我的女人 我只是同行的眼光 我祝满家的心间的雾一样 我想你的人 我会让海会给我一点\n","epoch 250, perplexity 17.180056, time 52.79 sec\n"," - 男人 不要人扶留在你的梦里 我看著你的身影 我要你的爱情 我的爱你的收 我不必孤单 我的心不是你的路我的\n"," - 女人 无法得到的天长 我们都在梦中解脱清醒的神吻 我要你的对方我的心 我们都在心中 一个归去 我会想番起\n"],"name":"stdout"}]}]}