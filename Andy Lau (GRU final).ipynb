{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Andy Lau (GRU final).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3aP2bEHUQjDPm2NmyItMS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cjaPAcw6ddnL","colab_type":"code","colab":{}},"source":["!pip install mxnet==1.6.0b20200101\n","!pip install d2lzh==0.8.10\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmvCNlDPcId1","colab_type":"code","outputId":"50efd130-fd6e-4eca-a74e-26485b78d2a3","executionInfo":{"status":"ok","timestamp":1585359992700,"user_tz":-480,"elapsed":3203,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import d2lzh as d2l\n","import math\n","from mxnet import autograd, gluon, init, nd\n","from mxnet.gluon import loss as gloss, nn, rnn\n","import time\n","\n","from mxnet import nd\n","import random\n","\n","with open('/content/drive/My Drive/Data/Andy_Lau.txt') as f:\n","  for line in f:\n","    corpus_chars = f.read()#.decode('utf-8')\n","\n","# This data set has many characters. For printing convenience, we replace line breaks with spaces\n","corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n","#corpus_chars = corpus_chars[0:10000]\n","\n","# We map each character to a continuous integer starting from 0, also known as the index, to facilitate subsequent data processing. \n","# In order to get the index, we take all the different characters in the data set, and then map them one by one to the index to construct the dictionary. \n","# Next, print vocab_size, which is the number of different characters in the dictionary, also known as the vocabulary size.\n","idx_to_char = list(set(corpus_chars))\n","char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n","vocab_size = len(char_to_idx)\n","corpus_indices = [char_to_idx[char] for char in corpus_chars]\n","\n","vocab_size"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2783"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"33_7lZ_Cd2lS","colab_type":"code","colab":{}},"source":["# Implementing from scratch\n","# Initializing model parameters\n","# The following code initializes the model parameters. The hyperparameter num_hiddens defines the number of hidden units.\n","\n","num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n","ctx = d2l.try_gpu()\n","\n","def get_params():\n","    def _one(shape):\n","        return nd.random.normal(scale=0.01, shape=shape, ctx=ctx)\n","\n","    def _three():\n","        return (_one((num_inputs, num_hiddens)),\n","                _one((num_hiddens, num_hiddens)),\n","                nd.zeros(num_hiddens, ctx=ctx))\n","\n","    W_xz, W_hz, b_z = _three()  # Update gate parameters\n","    W_xr, W_hr, b_r = _three()  # Reset gate parameters\n","    W_xh, W_hh, b_h = _three()  # Candidate hidden state parameters\n","    # Output layer parameters\n","    W_hq = _one((num_hiddens, num_outputs))\n","    b_q = nd.zeros(num_outputs, ctx=ctx)\n","    # Attach gradient\n","    params = [W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]\n","    for param in params:\n","        param.attach_grad()\n","    return params"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rFmvYuvd_Ph","colab_type":"code","colab":{}},"source":["# Defining the model\n","# The following code defines the hidden state initialization function init_gru_state. It returns a tuple consisting of an NDArray with a shape (batch size, number of hidden units) of 0.\n","\n","def init_gru_state(batch_size, num_hiddens, ctx):\n","    return (nd.zeros(shape=(batch_size, num_hiddens), ctx=ctx), )\n","\n","# Define the model according to the calculation expression of the GRU.\n","\n","def gru(inputs, state, params):\n","    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n","    H, = state\n","    outputs = []\n","    for X in inputs:\n","        Z = nd.sigmoid(nd.dot(X, W_xz) + nd.dot(H, W_hz) + b_z)\n","        R = nd.sigmoid(nd.dot(X, W_xr) + nd.dot(H, W_hr) + b_r)\n","        H_tilda = nd.tanh(nd.dot(X, W_xh) + nd.dot(R * H, W_hh) + b_h)\n","        H = Z * H + (1 - Z) * H_tilda\n","        Y = nd.dot(H, W_hq) + b_q\n","        outputs.append(Y)\n","    return outputs, (H,)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IG3WwNEeF7Q","colab_type":"code","outputId":"93b49f58-d8eb-49fb-d7c7-92bf82fb2e5f","executionInfo":{"status":"ok","timestamp":1585321159160,"user_tz":-480,"elapsed":26718946,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["# Train models and write lyrics\n","# Only use adjacent sampling when training the model. After setting the hyperparameters, we will train the model and create a 50-character piece of lyrics based on the prefixes \"男人\" and \"女人\".\n","\n","num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n","pred_period, pred_len, prefixes = 50, 50, ['男人', '女人']\n","\n","# Every 50 iterations we write a lyrics based on the currently trained model.\n","\n","d2l.train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,\n","                          vocab_size, ctx, corpus_indices, idx_to_char,\n","                          char_to_idx, False, num_epochs, num_steps, lr,\n","                          clipping_theta, batch_size, pred_period, pred_len,\n","                          prefixes)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 50, perplexity 22.993363, time 173.90 sec\n"," - 男人 就算是再见你不可再相恋 让我可以相爱过才是你 你的眼神就是我的家 有你的爱我的那一天你一生 也许不\n"," - 女人 就算是再见你不可再相恋 让我可以相爱过才是你 你的眼神就是我的家 有你的爱我的那一天你一生 也许不\n","epoch 100, perplexity 8.790387, time 168.05 sec\n"," - 男人 就算是分开不要得入爱 若要放弃了前途 难道出生的世界 我也会愿意在你地方的和我一个人的  在这里自\n"," - 女人 就算是分开不要得入爱 若要放弃了前途 难道出生的世界 我也会愿意在你地方的和我一个人的  在这里自\n","epoch 150, perplexity 5.255374, time 170.49 sec\n"," - 男人 但是你在这里 间空间一丝的美布 爱情在爱伤害了我愿去一点 如果你有天使怨怨会不敢 终于失去你从未爱\n"," - 女人 就算习惯一个人动人间 缘份就算再清楚 我会用去再可看看见已很想 现在我信爱已失踪 人不情爱你又会想\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X7Xjq-GoeQi2","colab_type":"code","outputId":"4a9f7433-092a-4983-c05e-5b565d50509e","executionInfo":{"status":"ok","timestamp":1585375575374,"user_tz":-480,"elapsed":10984150,"user":{"displayName":"Ned Li","photoUrl":"","userId":"07464141434226609575"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["# Simple implementation\n","# In Gluon, we can directly call the GRU class in the rnn module.\n","\n","num_epochs, num_steps, batch_size, lr, clipping_theta = 160, 35, 32, 1e2, 1e-2\n","pred_period, pred_len, prefixes = 50, 50, ['男人', '女人']\n","\n","gru_layer = rnn.GRU(num_hiddens)\n","model = d2l.RNNModel(gru_layer, vocab_size)\n","d2l.train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n","                                corpus_indices, idx_to_char, char_to_idx,\n","                                num_epochs, num_steps, lr, clipping_theta,\n","                                batch_size, pred_period, pred_len, prefixes)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch 50, perplexity 21.197166, time 96.85 sec\n"," - 男人生的 不会再一起一份 再会看一世间到的心 到底可到尽头 我有你的人 我会想番起 我会想番起 我会想番\n"," - 女人 我会想番起我由以前的环境转 到底怎么可以做错 我的心不可以像风雨 吹过千个人不可以 相信我爱情吧 \n","epoch 100, perplexity 7.914827, time 96.94 sec\n"," - 男人生 再见你一次我也不会想你 只要你在我的心里面看你的风里 说我都已在何地等待 我只想说不会的演 会慢\n"," - 女人 我会想番起我由以前嘅环境转变到而家 全部是无所谓 最爱你的人是我一个中国人 我也只能夜夜的望穿 那\n","epoch 150, perplexity 4.773199, time 96.75 sec\n"," - 男人哭吧 怎么可以给我女人 就算他看到半天才到在天边 让我可拥抱的火一 心想起争气还像在雪间 长夜多浪漫\n"," - 女人 我会想番起 我会想番起我由以前嘅环境转变到而家 这一生是我吗为何必再遇上 看著一切都不必会有人 可\n"],"name":"stdout"}]}]}